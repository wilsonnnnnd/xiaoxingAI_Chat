@echo off
chcp 65001 >nul
title å¯åŠ¨å°æ˜Ÿ APIï¼ˆllama-cpp-pythonï¼‰

echo [ğŸš€] æ­£åœ¨å¯åŠ¨å°æ˜Ÿæ¨¡å‹æœåŠ¡...

python -m llama_cpp.server ^
  --model E:\xiaoxing\llama.cpp\models\qwen1_5-7b-chat-q5_k_m.gguf ^
  --host 127.0.0.1 ^
  --port 8000 ^
  --n_ctx 4096 ^
  --n_gpu_layers 100 ^
  --n_threads 16 ^
  --chat_format chatml ^
  --cache true

REM âœ… å¯åŠ¨å¤±è´¥æ£€æµ‹
if %errorlevel% neq 0 (
    echo [âŒ] å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„ã€CUDA æ˜¯å¦å¯ç”¨ã€æ˜¯å¦ä¸º GPU ç‰ˆæœ¬ã€‚
    pause
    exit /b %errorlevel%
)

echo [âœ…] å°æ˜Ÿæ¨¡å‹æœåŠ¡å·²å…³é—­æˆ–é€€å‡ºã€‚
pause
